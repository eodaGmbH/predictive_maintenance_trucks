---
title: "Luftdrucksystemfehler bei Scania Trucks"
output: 
  flexdashboard::flex_dashboard:
    css: styles.css
    orientation: columns
    code_folding: hide
    vertical_layout: fill
---

```{r setup, eval = FALSE}
# library setup
library(flexdashboard)
library(magrittr)
library(dplyr)
library(caret)
library(purrr)
library(ggplot2)
library(stringr)
library(parallel)
library(doParallel)
library(Deriv)
library(caTools)
library(glmnet)
library(knitr)
library(kableExtra)
library(rmarkdown)

# global options
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)

# read csv data
data_trucks_training <- read.csv("aps_failure_training_set.csv", sep = ",", skip = 20, na.strings = "na")
data_trucks_test <- read.csv("aps_failure_test_set.csv", sep = ",", skip = 20, na.strings = "na")
```

Problematik
=======================================================================

Column {data-width=1}
-----------------------------------------------------------------------

### **Problemstellung** {data-height=700}
Der Scania-Trucks Datensatz kommt mit 171 anonymisierten Featuren sowie 76000 Observationen, welche im Verhältnis 60.000 zu 16.000 in Trainings- bzw. Testdaten aufgeteilt sind. Jede Observation enthält die Information, ob ein Fehler im Luftdruckssystem ( im Folgenden **LDS** genannt ) vorliegt oder nicht. <br><br>
Die Problemstellung lautet: <br>
<div class = "bordered">
*Sage für jede Observation im Testdatensatz voraus, ob ein Fehler im LDS vorliegt oder nicht. Minimiere dabei folgende Kostenfunktion:*
<br>
**$cost(FP, FN) = 10 \cdot FP + 500 \cdot FN$** <br>
*Es handelt sich hierbei also um ein **Klassifizierungsproblem**, mit der Kostenfunktion $cost(FP, FN)$ als Metrik.* <br>
</div> <br>
**<u>Fehler 1. Art (FP):</u>** <br>
Der Fehler 1. Art, hier **F**alse **P**ositive, bedeutet, dass man einer Observation einen Fehler im LDS zuschreibt, obwohl das Fahrzeug fehlerfrei ist. Man schickt das Fahrzeug also in die Werkstatt, obwohl kein Fehler vorhanden ist. <br><br>
**<u>Fehler 2. Art (FN):</u>** <br>
Der Fehler 2. Art, hier **F**alse **N**egative, bedeutet, dass man eine Observation als fehlerfrei einstuft, obwohl das Fahrezeug einen Fehler im LDS aufweist. <br><br>
<div class = "bordered">
*Der Fehler 1. Art ist logischerweise deutlich kostengünstiger gewichtet als der Fehler 2. Art. Es ist wesentlich günstiger einen LKW fälschlicherweise in die Werkstatt zur Überprüfung zu schicken, als dass ein LKW, bedingt durch einen LDS-Fehler, auf der Strecke liegen bleibt.*
</div> <br><br>

### **Hintergrund** {data-height=300}
Das Scania-Trucks Problem wurde auf der **IDA 2016** vorgestellt. Die IDA ist ein internationales Symposium über intelligente Datenanalyse, welche 2016 im Kontext des **maschinellen Lernens** das Industrieproblem für die Öffentlichkeit zugänglich gemacht hat. Scania, Mitsponsor der IDA, hat auf die beiden kostengünstigstens Modelle einen Preis von 500$ aussgeschrieben, sowie einen Rednerspot auf der **IDA** selbst. <br><br>

<div id = "bordered> <img src = "Scania.png" alt = "Scania Logo" height = "100%" " width = "100%"></div>

Column {data-width=3}
----------------------------------------------------------------------

###
<center>
<div style = "margin-top: 15vh"> <img src = "chart.png" alt = "Chart" height = "100%" " width = "70%"></div>
</center>
<br><br><br><br><br><br>
<div style = "float: right; text-align: right;"> <img src = "eoda_logo.png" alt = "Eoda Logo" height = "25%" " width = "25%"></div>

Datensatz {data-orientation=rows}
===============================================

Row {data-height=200}
-----------------------------------------------

### **Histogramm-Feature**
Aus der Beschreibung des Scnaia-Trucks Problems lässt sich herauslesen, dass gewisse Feature Histogrammdaten abbilden. 
Das heißt, dass ein Feature auf mehrere Container aufgeteilt ist, wobei der Wert im Container $A_i$ die Aufenthaltsdauer in der Klasse $i$ bedeutet, wobei $A$ das obergeordnete Feature ist. <br>
Beispielverteilung des Features **ag**:

```{r table_hist_ag}
# Example-table for the histogramm-feature
kable(data_trucks_training[1:8, str_detect(colnames(data_trucks_training), "ag_00+") == TRUE], "html", caption = "Histogrammdaten") %>%
  kable_styling("striped")
```

Row {data-height=300 .tabset .tabset-fade}
-----------------------------------------------

### Feature **ag**

```{r mean_hist_ag}
# calculate mean distribution for the ag feature
ag_labels <- paste0("ag_00", 0:9)
ag_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "ag_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
ag_mean_df <- data.frame(labels = ag_labels, mean = ag_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_ag, out.width = "95%", fig.height = 3}
# plot ag distribution
ggplot(ag_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_ag_log, out.width = "95%", fig.height = 3}
# plot log(ag) distribution
ggplot(ag_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **ay**

```{r mean_hist_ay}
# calculate mean distribution for the ay feature
ay_labels <- paste0("ay_00", 0:9)
ay_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "ay_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
ay_mean_df <- data.frame(labels = ay_labels, mean = ay_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_ay, out.width = "95%", fig.height = 3}
# plot ay distribution
ggplot(ay_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_ay_log, out.width = "95%", fig.height = 3}
# plot log(ay) distribution
ggplot(ay_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **az**

```{r mean_hist_az}
# calculate mean distribution for the az feature
az_labels <- paste0("az_00", 0:9)
az_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "az_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
az_mean_df <- data.frame(labels = az_labels, mean = az_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_az, out.width = "95%", fig.height = 3}
# plot az distribution
ggplot(az_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_az_log, out.width = "95%", fig.height = 3}
# plot log(az) distribution
ggplot(az_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **ba**

```{r mean_hist_ba}
# calculate mean distribution for the ba feature
ba_labels <- paste0("ba_00", 0:9)
ba_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "ba_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
ba_mean_df <- data.frame(labels = ba_labels, mean = ba_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_ba, out.width = "95%", fig.height = 3}
# plot ba distribution
ggplot(ba_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_ba_log, out.width = "95%", fig.height = 3}
# plot log(ba) distribution
ggplot(ba_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **cn**

```{r mean_hist_cn}
# calculate mean distribution for the cn feature
cn_labels <- paste0("cn_00", 0:9)
cn_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "cn_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
cn_mean_df <- data.frame(labels = cn_labels, mean = cn_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_cn, out.width = "95%", fig.height = 3}
# plot cn distribution
ggplot(cn_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_cn_log, out.width = "95%", fig.height = 3}
# plot log(cn) distribution
ggplot(cn_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **cs**

```{r mean_hist_cs}
# calculate mean distribution for the cs feature
cs_labels <- paste0("cs_00", 0:9)
cs_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "cs_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
cs_mean_df <- data.frame(labels = cs_labels, mean = cs_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_cs, out.width = "95%", fig.height = 3}
# plot cs distribution
ggplot(cs_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_cs_log, out.width = "95%", fig.height = 3}
# plot log(cs) distribution
ggplot(cs_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

### Feature **ee**

```{r mean_hist_ee}
# calculate mean distribution for the ee feature
ee_labels <- paste0("ee_00", 0:9)
ee_mean <- sapply(data_trucks_training[, str_detect(colnames(data_trucks_training), "ee_00+") == TRUE], function(x) mean(x, na.rm = TRUE)) %>% as.numeric()
ee_mean_df <- data.frame(labels = ee_labels, mean = ee_mean)
```


<div style = "display: inline-block; width: 45vw; margin-top: 4vh;">
```{r plot_ee, out.width = "95%", fig.height = 3}
# plot ee distribution
ggplot(ee_mean_df, aes(x = labels, y = mean)) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  theme_minimal() +
  ggtitle("Verteilung des Mittelwertes") +
  xlab("") + ylab("")
```
</div>

<div style = "display: inline-block; width: 45vw; margin-top: 4vh; margin-left: 5vw;">
```{r plot_ee_log, out.width = "95%", fig.height = 3}
# plot log(ee) distribution
ggplot(ee_mean_df, aes(x = labels, y = log(mean))) +
  geom_bar(stat = "identity", fill = "#88ab33") +
  scale_y_continuous(trans = "log10") +
  theme_minimal() +
  ggtitle("Verteilung von log(Mittelwert)") +
  xlab("") + ylab("")
```
</div>

Feature-Engineering
=========================================

Column {data-width=100}
-----------------------------------------

### **NA-Analyse**

Der Scania-Trucks Datensatz weist eine hohe Zahl an fehlenden Werten ( hier im Folgenden **NA's** genannt) auf. Dies kann insbesondere beim modellieren zu Problemen führen, da fehlende Werte für den **Klassifizierungsalgorithmus** unmöglich zu handlen sind.
Eine Möglichkeit besteht darin, eine gewisse Anzahl an NA's zuzulassen und demnach die Anzahl der Feature anhand dieser Grenze zu reduzieren. Alle Observationen, welche nach der **Featurereduktion** noch NA's enthalten, werden entfernt. <br>
Folgende Tabelle liefert einen Überblick über die **NA Grenzen**: <br>

```{r NA_data_frame, eval = FALSE}
# setup the whole dataset
data_trucks <- rbind(data_trucks_training, data_trucks_test)

# generate data frame for the na boundaries
na_df <- data.frame(Var = colnames(data_trucks), 
                    Na = sapply(data_trucks, function(x) sum(is.na(x))))

# get unique na breakpoints for all features
na_breaks <- na_df$Na %>% unique() %>% sort()

result_NA <- data_frame(na_number = na_breaks, vars = numeric(length = length(na_breaks)),
                        obs = numeric(length = length(na_breaks)),
                        obs_pos = numeric(length(na_breaks)))
# calculate the feature_gain / observation_loss for all na boundaries
for (i in na_breaks){
  
  select_columns <- 
    which(na_df$Na <= i) %>% 
    as.numeric()
  
  sub_trucks <- data_trucks[ , select_columns] %>% na.omit()
  
  n_obs <- nrow(sub_trucks)
  n_vars <- ncol(sub_trucks)
  n_obs_pos <- sub_trucks[sub_trucks$class == "pos", ] %>% count() %>% as.numeric()
  
  result_NA$vars[result_NA$na_number == i] = n_vars
  result_NA$obs[result_NA$na_number == i] = n_obs
  result_NA$obs_pos[result_NA$na_number == i] = n_obs_pos
}

# get the relative gain / loss
result_NA <- result_NA %>%
  mutate(vars_percent = (vars / ncol(data_trucks)) * 100, 
         obs_percent = (obs / nrow(data_trucks)) * 100,
         obs_pos_percent = (obs_pos / nrow(data_trucks[data_trucks$class == "pos", ])) * 100)
```

<br>

```{r NA_table}
# print table for the na boundaries
kable(result_NA %>% head(5), "html", caption = "NA Grenzen, Featureanzahl ( Grün ), allgemeiner Obsservationsverlust ( Orange ) & fehlerhafter Observationsverlust ( Blau )") %>%
  kable_styling("striped") %>%
  column_spec(c(2,5), background = "#88ab33", color = "white") %>%
  column_spec(c(3,6), background = "#F98948", color = "white") %>%
  column_spec(c(4,7), background = "#437F97", color = "white") 
```

<br>

<center>
```{r NA_plot}
# plot the na boundaries respective to the gain / loss
ggplot(result_NA) + 
  geom_point(aes(na_number, vars_percent), color = "#88ab33") +
  geom_point(aes(na_number, obs_percent), color = "#F98948") +
  geom_point(aes(na_number, obs_pos_percent), color = "#437F97") +
  xlab("Number of NA's") + ylab("") +
  scale_y_continuous(limits = c(1, 100)) 
```
</center>

<br>

<div class = "bordered">
<center>**Featuregewinnung:**</center> <br>
Aus der Tabelle lässt sich erkennen, dass bei den unterschiedlichen NA-Grenzen die Anzahl an gewonnener Feature stark schwankt. So gewinnt man beim Sprung von 0 auf 195 NA's lediglich 1 Feature dazu, beim Sprung von 861 auf 863 Feature aber direkt 30 Feature. Das NA verhalten ist also alles andere als proportional. Die Entscheidung, welche NA-Grenze man als Auschlussverfahren nimmt, hängt aber nicht nur von der Featuregewinnung ab.
</div> <br>
<div class = "bordered">
<center>**Allgemeiner Observationsverlust:**</center> <br>
Alle Feature im späteren Modell zu berücksichten bringt nichts, wenn man nur noch auf 5% der ursprünglichen Daten trainieren kann. Deshalb spielt der **Observationsverlust** eine starke Rolle in der Entscheidung. Hier zeichnet sich ein starker Sprung von 5598 auf 12012 NA's ab. Evaluiert man also die Grenzen anhand des Observationsverlustes, sollte man nicht über die Grenze von 5598 hinaus gehen, da direkt danach c.a. 10% der LKW's zusätzlich verloren gehen. 
</div> <br>
Normalerweise wären dass bereits ausreichende Informationen, um eine fundierte Meinung über die Wahl der NA-Grenze zu treffen. Der Scania-Trucks Datensatz weist allerdings ein noch größeres Problem auf. Von den 76000 Observationen sind lediglich **1375** fehlerhaft. Dadurch könnte es passieren, dass wir bei der Wahl der NA-Grenze alle fehlerhaften Observationen verlieren. <br>
Demnach muss zusätzlich folgende Variable mit einkalkuliert werden: <br><br>
<div class = "bordered">
<center>**Fehlerhafter Observationsverlust:**</center> <br>
Der fehlerhafte Oberservationsverlust beschreibt die Anzahl der Observationen die bei der Wahl der NA-Grenze verloren gehen und zusätzlich einen Fehler im LDS aufweisen. Anhand der geringen Fehlerrate im Datensatz wäre es fatal, zu viele der fehlerhaften Fahrzeuge zu verlieren, da ein Modell so möglicherweise auf ausschließlich fehlerfreien Daten trainieren könnte. Die Grenze für fehlerhafte Observationen findet sich in der Tabelle offensichtlich beim Sprung von 928 NA's auf 3188 NA's. Obwohl man hier nur 1 Feature dazu gewinnen würde, fallen 30% der fehlerhaften Observationen dadurch raus. Die Grenze ist demnach also bei 928 NA's zu setzen.
</div> </br>
<center>**<u>Fazit:</u>**</center>
Die NA-Analyse zeigt, dass verschiedene Breakpoints existieren, um eine gewisse Anzahl von Featuren zu erhalten / nicht zu viele Observationen zu verlieren. Um auf dem kompletten Trainingsset vorhersagen treffen zu können, müssen die fehlenden Werte aber aufgefüllt werden, anstatt **NA-Reduktion** zu betreiben. Hierzu stehen Möglichkeiten wie **knn-imputation** oder **median-imputation** zur Verfügung. Im Folgenden werden diese Möglichkeiten evaluiert.




Column {data-width=100}
-----------------------------------------

### **Datenaufbereitung ( Relativierung )**


Die **Datensatzanalyse** hat gezeigt, dass viele der Histogrammdaten nicht gleichmäßig ( normal- ) verteilt sind.
Oftmals sind Container eines Histogrammfeatures für fast alle Observationen 0, andere Containere hingegen sind mit den unterschiedlichsten Größenordnungen von Zahlen gefüllt. <br>
Hinzu kommt, dass die Werte in den verschiedenen Containern **absolute** Daten sind, das heißt: <br><br>
<div class = "bordered" style = "width: 30vw; font-size: 20px;">
**$\sum_{i = 0}^{9}{A_i}$** <br><br>
ist **nicht notwendigerweise gleich** für alle Observationen.
</div> 

<br>

Um das Problem der absoluten Histogrammdaten zu beseitigen ist es sinnvoll diese zu **relativieren**. Man stellt also jeden absoluten Wert $i$ in **prozentuellen Bezug** zu der **Gesamtaufenthaltszeit** jeder Observation in der Klasse $A$. <br>
Dadurch ergibt sich folgende Formel für die neuen Werte $\tilde{A}$: <br><br>
<div class = "bordered" style = "font-size: 20px; width: 30vw;">
**$\tilde{A}_i = \frac{A_i}{\sum_{i = 0}^{9}{A_i}}  mit  \sum_{i = 0}^{9}{\tilde{A}_i} = 1$** <br><br>
für **jede** Observation.
</div>
<br>

**<u>Vorher:</u>** <br>

```{r hist_ag_before_table}
# example table of the histogramm feature  ag before qualifying the data
kable(data_trucks_training[1:5, str_detect(colnames(data_trucks_training), "ag_00+") == TRUE], "html", caption = "Histogrammdaten") %>%
  kable_styling("striped")
```

```{r histogramm_percentage, eval = FALSE}
# qualify the histogramm data
data_trucks_perc <- data_trucks %>%
  mutate(
    perc_ag_0 = ag_000 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_1 = ag_001 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_2 = ag_002 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_3 = ag_003 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_4 = ag_004 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_5 = ag_005 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_6 = ag_006 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_7 = ag_007 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_8 = ag_008 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ag_9 = ag_009 / (ag_000 + ag_001 + ag_002 + ag_003 + ag_004 + ag_005 + ag_006 + ag_007 + ag_008 + ag_009),
    perc_ay_0 = ay_000 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_1 = ay_001 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_2 = ay_002 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_3 = ay_003 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_4 = ay_004 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_5 = ay_005 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_6 = ay_006 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_7 = ay_007 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_8 = ay_008 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_ay_9 = ay_009 / (ay_000 + ay_001 + ay_002 + ay_003 + ay_004 + ay_005 + ay_006 + ay_007 + ay_008 + ay_009),
    perc_az_0 = az_000 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_1 = az_001 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_2 = az_002 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_3 = az_003 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_4 = az_004 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_5 = az_005 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_6 = az_006 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_7 = az_007 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_8 = az_008 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_az_9 = az_009 / (az_000 + az_001 + az_002 + az_003 + az_004 + az_005 + az_006 + az_007 + az_008 + az_009),
    perc_ba_0 = ba_000 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_1 = ba_001 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_2 = ba_002 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_3 = ba_003 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_4 = ba_004 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_5 = ba_005 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_6 = ba_006 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_7 = ba_007 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_8 = ba_008 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_ba_9 = ba_009 / (ba_000 + ba_001 + ba_002 + ba_003 + ba_004 + ba_005 + ba_006 + ba_007 + ba_008 + ba_009),
    perc_cn_0 = cn_000 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_1 = cn_001 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_2 = cn_002 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_3 = cn_003 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_4 = cn_004 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_5 = cn_005 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_6 = cn_006 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_7 = cn_007 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_8 = cn_008 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cn_9 = cn_009 / (cn_000 + cn_001 + cn_002 + cn_003 + cn_004 + cn_005 + cn_006 + cn_007 + cn_008 + cn_009),
    perc_cs_0 = cs_000 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_1 = cs_001 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_2 = cs_002 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_3 = cs_003 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_4 = cs_004 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_5 = cs_005 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_6 = cs_006 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_7 = cs_007 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_8 = cs_008 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_cs_9 = cs_009 / (cs_000 + cs_001 + cs_002 + cs_003 + cs_004 + cs_005 + cs_006 + cs_007 + cs_008 + cs_009),
    perc_ee_0 = ee_000 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_1 = ee_001 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_2 = ee_002 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_3 = ee_003 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_4 = ee_004 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_5 = ee_005 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_6 = ee_006 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_7 = ee_007 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_8 = ee_008 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009),
    perc_ee_9 = ee_009 / (ee_000 + ee_001 + ee_002 + ee_003 + ee_004 + ee_005 + ee_006 + ee_007 + ee_008 + ee_009)
  )

data_trucks_perc <- data_trucks_perc[, !str_detect(names(data_trucks_perc), "ag_00+|ay_00+|az_00+|ba_00+|cn_00+|cs_00+|ee_00+")]
```

<br>

**<u>Nachher:</u>** <br>

```{r hist_ag_after_table}
# example table of the histogramm feature ag after qualifying the data
kable(data_trucks_perc[1:5, str_detect(colnames(data_trucks_perc), "perc_ag_+") == TRUE], "html", caption = "Histogrammdaten") %>%
  kable_styling("striped", position = "left" )
```

Column {data-width=100}
-----------------------------------------

### **Pre-Processing**

```{r Pre-Process, eval = FALSE}
# setup the knn imputation
data_trucks_knn <- preProcess(data_trucks_perc, method = "knnImpute")

data_trucks_knn_training <- predict(data_trucks_knn, data_trucks_perc[1:60000, ])
data_trucks_knn_test <- predict(data_trucks_knn, data_trucks_perc[60001:76000, ])
```

Zusätzlich zu seinen Histogrammdaten verfügt der Datensatz über, wie bereits erwähnt, eine vielzahl von **NA's** ( fehlende Werte ).
Diese würden bereits beim trainieren der Modelle für Probleme sorgen, da der **Algorithmus** mit diesen Daten nicht arbeiten kann ( keine Splits setzen, Regressionskoeffizienten berechnen, etc. ). <br>
Eine Möglichkeit bietet hier die **knn-Imputation** ( *k-nächste-Nachbarn-Schätzung* ). <br><br>
Die Schätzung erfolgt in **3** Schritten. <br>
<center>
<font size = "13px"><font color = "#88ab33">Zentrieren</font>, <font color = "#F98948">Skalieren</font> & <font color = "#437F97">Schätzen</font></font>
</center>
<br>
Folgende Tabellenausschnitte verdeutlicht diese Schritte: <br><br>

<div style = "display: inline;">

<div style = "display: inline-block; width: 49%;">
```{r standard_table}
# example table before preProcessing 
kable(data_trucks[["ad_000"]] %>% head(5), "html") %>%
  kable_styling("striped") 
```
</div>

<div style = "display: inline-block; width: 49%;">
```{r center_table}
# example table after centering
kable((data_trucks[["ad_000"]] - mean(data_trucks[["ad_000"]], na.rm = TRUE)) %>% head(5), "html") %>%
  kable_styling("striped") %>%
  column_spec(1, background = "#88ab33", color = "white")
```
</div>

</div>

<div style = "display: inline;">

<div style = "display: inline-block; width: 49%;">
```{r scale_table}
# example table after centering and scaling
kable(((data_trucks[["ad_000"]] - mean(data_trucks[["ad_000"]], na.rm = TRUE)) / sd(data_trucks[["ad_000"]], na.rm = TRUE)) %>% head(5), "html") %>%
  kable_styling("striped") %>% 
  column_spec(1, background = "#F98948", color = "white")
```
</div>

<div style = "display: inline-block; width: 49%">
```{r impute_table}
# example table after centering, scaling and knn imputation
kable(data_trucks_knn_training[["ad_000"]] %>% head(5), "html") %>%
  kable_styling("striped") %>%
  column_spec(1, background = "#437F97", color = "white")
```
</div>

</div>


Modellierung
===============================

```{r evaluation_functions, eval = FALSE}
# helperfunctions for cost evaluation und sensitivity / specificity calculation for given thresholds
sensspec_rate <- function(threshold, predicted_set, test_set) {
  predicted_class <- ifelse(predicted_set$neg > threshold, "neg", "pos")
  return(confusionMatrix(predicted_class, test_set$class)$table %>% data.frame() %>% extract2("Freq"))
}

min_cost_func <- function(threshold_vec, predicted_set, test_set) {
  false_negatives <- sapply(threshold_vec, function(x) sensspec_rate(x, predicted_set, test_set)[3])
  false_positives <- sapply(threshold_vec, function(x) sensspec_rate(x, predicted_set, test_set)[2])

  threshold_df <- data.frame(threshold = threshold_vec,
                             false_positives = false_positives,
                             false_negatives = false_negatives)
  
  threshold_df <- threshold_df %>%
    mutate(
      cost = false_positives * 10 + false_negatives * 500
    )
  
  return(threshold_df)
}
```


Column {data-width=100}
------------------------------

### **Modellauswahl & Güte**

Zur Lösung des **Klassifizierungsproblems** bietet sich ein **Random-Forest-Modell** an. Als **Parameterraster** werden folgende Werte benutzt:
<div class = "bordered">
<center>$mtry = \sqrt{171} \approx 13$</center> <br>
*Als **mtry** verwenden die Modelle den Standardwert aus $\sqrt{Anzahl \space der \space Feature}$.* <br><br>
<center>$splitrule = gini$</center> <br>
*Als **splitrule** verwenden die Modelle den **Gini-Index** für Klassifikationsprobleme.*
</div> 

<br><br>

<center>**<u>Modell 1:</u> 1-fache, 5-faltige Kreuzvalidierung mit 2500 Bäumen**</center> <br>

```{r rf_model_cv1_2.5k, eval = FALSE}
# model 1 - random forest with 1 times 5 folded cross validation and 2500 trees
control_knn_cv_five_one <- trainControl(method = "repeatedcv",
                                        number = 5,
                                        repeats = 1,
                                        allowParallel = TRUE,
                                        classProbs = TRUE,
                                        verboseIter = TRUE,
                                        summaryFunction = mnLogLoss)

grid_knn <- expand.grid(mtry = 13, splitrule = "gini", min.node.size = 1)

cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

system.time(
  model_knn_cv1_2.5k <- train(class ~ .,
                              data = data_trucks_knn_training,
                              method = "ranger",
                              metric = "logLoss",
                              importance = "impurity",
                              verbose = TRUE,
                              trControl = control_knn_cv_five_one,
                              tuneGrid = grid_knn,
                              num.trees = 2500)
)

stopCluster(cluster)
registerDoSEQ()

```

```{r evaluation_cv1_2.5k, eval = FALSE}
# get optimal classificationthreshold to minimize the cost function for model 1
pred_cv1_2.5k <- predict(model_knn_cv1_2.5k, data_trucks_knn_test, type = "prob")

threshold_df_cv1_2.5k <- min_cost_func(seq(from = 0.95, to = 0.96, length.out = 100), pred_cv1_2.5k, data_trucks_knn_test)
min_cost_place_cv1_2.5k <- which(threshold_df_cv1_2.5k$cost == min(threshold_df_cv1_2.5k$cost))[1]
min_threshold_cv1_2.5k <- threshold_df_cv1_2.5k$threshold[min_cost_place_cv1_2.5k]
min_cost_cv1_2.5k <- threshold_df_cv1_2.5k$cost[min_cost_place_cv1_2.5k]
ggplot(threshold_df_cv1_2.5k, aes(x = threshold_df_cv1_2.5k$threshold, y = threshold_df_cv1_2.5k$cost)) +
  geom_line()
pred_cv1_2.5k_class <- ifelse(pred_cv1_2.5k$neg >= min_threshold_cv1_2.5k, "neg", "pos")
```

<div style = "display: inline">

<div style = "display: inline-block; vertical-align: text-top;>
```{r confusion_cv1_2.5k_display}
# display the confusion Matrix of model 1
confusionMatrix(pred_cv1_2.5k_class, data_trucks_knn_test$class)
```
</div>
<div style = "display: inline-block; vertical-align: text-top;">
```{r model_cv1_2.5k_display}
# display model 1
model_knn_cv1_2.5k
```
<br>
<center>
<u>***Ergebnis:***</u> <br><br>
***Kosten:** `r min_cost_cv1_2.5k`* <br><br>
***Kosten pro LKW:** `r min_cost_cv1_2.5k / nrow(data_trucks_knn_test)`*
</center>
</div>

</div>

<br><br>

<center>**<u>Modell 2:</u> 10-fache, 5-faltige Kreuzvalidierung mit 500 Bäumen**</center> <br>

```{r rf_model_cv10_0.5k, eval = FALSE}
# model 2 - random forest with 10 times 5 folded cross validation and 500 trees
control_knn_cv_five_ten <- trainControl(method = "repeatedcv",
                                        number = 5,
                                        repeats = 10,
                                        allowParallel = TRUE,
                                        classProbs = TRUE,
                                        verboseIter = TRUE,
                                        summaryFunction = mnLogLoss)

grid_knn <- expand.grid(mtry = 13, splitrule = "gini", min.node.size = 1)

cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

system.time(
  model_knn_cv10_0.5k <- train(class ~ .,
                               data = data_trucks_knn_training,
                               method = "ranger",
                               metric = "logLoss",
                               importance = "impurity",
                               verbose = TRUE,
                               trControl = control_knn_cv_five_ten,
                               tuneGrid = grid_knn,
                               num.trees = 500)
)

stopCluster(cluster)
registerDoSEQ()

```

```{r evaluation_cv10_0.5k, eval = FALSE}
# get optimal classificationthreshold to minimize the cost function for model 2
pred_cv10_0.5k <- predict(model_knn_cv10_0.5k, data_trucks_knn_test, type = "prob")

threshold_df_cv10_0.5k <- min_cost_func(seq(from = 0.93, to = 0.95, length.out = 100), pred_cv10_0.5k, data_trucks_knn_test)
min_cost_place_cv10_0.5k <- which(threshold_df_cv10_0.5k$cost == min(threshold_df_cv10_0.5k$cost))[1]
min_threshold_cv10_0.5k <- threshold_df_cv10_0.5k$threshold[min_cost_place_cv10_0.5k]
min_cost_cv10_0.5k <- threshold_df_cv10_0.5k$cost[min_cost_place_cv10_0.5k]
ggplot(threshold_df_cv10_0.5k, aes(x = threshold_df_cv10_0.5k$threshold, y = threshold_df_cv10_0.5k$cost)) +
  geom_line()
pred_cv10_0.5k_class <- ifelse(pred_cv10_0.5k$neg >= min_threshold_cv10_0.5k, "neg", "pos")
```

<div style = "display: inline;">

<div style = "display: inline-block; vertical-align: text-top;">
```{r confusion_cv10_0.5k_display}
# display the confusion Matrix of model 2
confusionMatrix(pred_cv10_0.5k_class, data_trucks_knn_test$class)
```
</div>
<div style = "display: inline-block; vertical-align: text-top;">
```{r model_cv10_0.5k_display}
# display model 2
model_knn_cv10_0.5k
```
<center>
<br>
<u>***Ergebnis:***</u> <br><br>
***Kosten:** `r min_cost_cv10_0.5k`* <br><br>
***Kosten pro LKW:** `r min_cost_cv10_0.5k / nrow(data_trucks_knn_test)`*
</center>
</div>

</div>

<br><br>

<center>**<u>Modell 3:</u> 10-fache, 5-faltige Kreuzvalidierung mit 1000 Bäumen**</center> <br>

```{r rf_model_cv10_1k, eval = FALSE}
# model 3 - random forest with 10 times 5 folded cross validation with 1000 trees
control_knn_cv_five_ten <- trainControl(method = "repeatedcv",
                                        number = 5,
                                        repeats = 10,
                                        allowParallel = TRUE,
                                        classProbs = TRUE,
                                        verboseIter = TRUE,
                                        summaryFunction = mnLogLoss)

grid_knn <- expand.grid(mtry = 13, splitrule = "gini", min.node.size = 1)

cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

system.time(
  model_knn_cv10_1k <- train(class ~ .,
                             data = data_trucks_knn_training,
                             method = "ranger",
                             metric = "logLoss",
                             importance = "impurity",
                             verbose = TRUE,
                             trControl = control_knn_cv_five_ten,
                             tuneGrid = grid_knn,
                             num.trees = 1000)
)

stopCluster(cluster)
registerDoSEQ()

```

```{r evaluation_cv10_1k, eval = FALSE}
# get optimal classificationthreshold to minimize the cost function for model 3
pred_cv10_1k <- predict(model_knn_cv10_1k, data_trucks_knn_test, type = "prob")

threshold_df_cv10_1k <- min_cost_func(seq(from = 0.97, to = 0.98, length.out = 100), pred_cv10_1k, data_trucks_knn_test)
min_cost_place_cv10_1k <- which(threshold_df_cv10_1k$cost == min(threshold_df_cv10_1k$cost))[1]
min_threshold_cv10_1k <- threshold_df_cv10_1k$threshold[min_cost_place_cv10_1k]
min_cost_cv10_1k <- threshold_df_cv10_1k$cost[min_cost_place_cv10_1k]
ggplot(threshold_df_cv10_1k, aes(x = threshold_df_cv10_1k$threshold, y = threshold_df_cv10_1k$cost)) +
  geom_line()
pred_cv10_1k_class <- ifelse(pred_cv10_1k$neg >= min_threshold_cv10_1k, "neg", "pos")
```

<div style = "display: inline;">

<div style = "display: inline-block; vertical-align: text-top;">
```{r confusion_cv10_1k_display}
# display confusion Matrix of model 3
confusionMatrix(pred_cv10_1k_class, data_trucks_knn_test$class)
```
</div>
<div style = "display: inline-block; vertical-align: text-top;">
```{r model_cv10_1k_display}
# display model 3
model_knn_cv10_1k
```
<center>
<br>
<u>***Ergebnis:***</u> <br><br>
***Kosten:** `r min_cost_cv10_1k`* <br><br>
***Kosten pro LKW:** `r min_cost_cv10_1k / nrow(data_trucks_knn_test)`*
</center>
</div>

</div>

<br><br>

<center>**<u>Modell 4:</u> 10-fache, 5-faltige Kreuzvalidierung mit 2500 Bäumen**</center> <br>

```{r rf_model_cv10_2.5k, eval = FALSE}
# model 4- random forest with 10 times 5 folded cross validation with 2500 trees
control_knn_cv_five_ten <- trainControl(method = "repeatedcv",
                                        number = 5,
                                        repeats = 10,
                                        allowParallel = TRUE,
                                        classProbs = TRUE,
                                        verboseIter = TRUE,
                                        summaryFunction = mnLogLoss)

grid_knn <- expand.grid(mtry = 13, splitrule = "gini", min.node.size = 1)

cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

system.time(
  model_knn_cv10_2.5k <- train(class ~ .,
                               data = data_trucks_knn_training,
                               method = "ranger",
                               metric = "logLoss",
                               importance = "impurity",
                               verbose = TRUE,
                               trControl = control_knn_cv_five_ten,
                               tuneGrid = grid_knn,
                               num.trees = 2500)
)

stopCluster(cluster)
registerDoSEQ()

```

```{r evaluation_cv10_2.5k, eval = FALSE}
# get optimal classification threshold to minimize the cost function for model 4
pred_cv10_2.5k <- predict(model_knn_cv10_2.5k, data_trucks_knn_test, type = "prob")

threshold_df_cv10_2.5k <- min_cost_func(seq(from = 0.95, to = 0.96, length.out = 100), pred_cv10_2.5k, data_trucks_knn_test)
min_cost_place_cv10_2.5k <- which(threshold_df_cv10_2.5k$cost == min(threshold_df_cv10_2.5k$cost))[1]
min_threshold_cv10_2.5k <- threshold_df_cv10_2.5k$threshold[min_cost_place_cv10_2.5k]
min_cost_cv10_2.5k <- threshold_df_cv10_2.5k$cost[min_cost_place_cv10_2.5k]
ggplot(threshold_df_cv10_2.5k, aes(x = threshold_df_cv10_2.5k$threshold, y = threshold_df_cv10_2.5k$cost)) +
  geom_line()
```

<div style = "display: inline;">

<div style = "display: inline-block; vertical-align: text-top;">
```{r confusion_cv10_2.5k_display}
# display confusion Matrix of model 4
confusionMatrix(pred_cv10_2.5k_class, data_trucks_knn_test$class)
```
</div>
<div style = "display: inline-block; vertical-align: text-top;">
```{r model_cv10_2.5k_display}
# display model 4
model_knn_cv10_1k
```
<center>
<br>
<u>***Ergebnis:***</u> <br><br>
***Kosten:** `r min_cost_cv10_2.5k`* <br><br>
***Kosten pro LKW:** `r min_cost_cv10_2.5k / nrow(data_trucks_knn_test)`*
</center>
</div>

</div>

Column {data-width=100}
------------------------------

### **Variablenwichtigkeit & Laufzeit**

<center>**<u>Specs:</u>**</center>

<center>
*Intel(R) Core(TM) i5-6200U CPU @ 2.30GHz, 2400 MHz, 2 Kerne, 4 logische Prozessoren, 8GB RAM* <br>
*Gerechnet wurde auf 3 Prozessoren*
</center>

<br>

<center>**<u>Modell 1:</u>**</center> <br>

```{r variable_importance_cv1_2.5k, eval = FALSE}
# get variable importance of model 1
var_imp_df_cv1_2.5k <- data.frame(Variable = rownames(varImp(model_knn_cv1_2.5k)[[1]]), Importance = varImp(model_knn_cv1_2.5k)[[1]]$Overall,
                                  Rank = rank(varImp(model_knn_cv1_2.5k)[[1]]$Overall)) %>%
  arrange(desc(Rank))
```

```{r var_imp_cv1_2.5k_table}
# display variable importance of model 1
kable(var_imp_df_cv1_2.5k[, 1:2] %>% head(5), "html") %>%
  kable_styling("striped")
```

<center>**$Laufzeit \approx 45 \space Minuten$**</center>

<br><br>

<center>**<u>Modell 2:</u>**</center> <br>

```{r variable-_importance_cv10_0.5k, eval = FALSE}
# get variable importance of model 2
var_imp_df_cv10_0.5k <- data.frame(Variable = rownames(varImp(model_knn_cv10_0.5k)[[1]]), Importance = varImp(model_knn_cv10_0.5k)[[1]]$Overall,
                                  Rank = rank(varImp(model_knn_cv10_0.5k)[[1]]$Overall)) %>%
  arrange(desc(Rank))
```

```{r var_imp_cv10_0.5k_table}
# display variable importance of model 2
kable(var_imp_df_cv10_0.5k[, 1:2] %>% head(5), "html") %>%
  kable_styling("striped")
```

<center>**$Laufzeit \approx 1 \space Stunde \space 11 \space Minuten$**</center>

<br><br>

<center>**<u>Modell 3:</u>**</center> <br>

```{r variable_importance_cv10_1k, eval = FALSE}
# get variable importance of model 3
var_imp_df_cv10_1k <- data.frame(Variable = rownames(varImp(model_knn_cv10_1k)[[1]]), Importance = varImp(model_knn_cv10_1k)[[1]]$Overall,
                                  Rank = rank(varImp(model_knn_cv10_1k)[[1]]$Overall)) %>%
  arrange(desc(Rank))
```

```{r var_imp_cv10_1k_table}
# display variable importance of model 3
kable(var_imp_df_cv10_1k[, 1:2] %>% head(5), "html") %>%
  kable_styling("striped")
```

<center>**$Laufzeit \approx 2 \space Stunden \space 21 \space Minuten$**</center>

<br><br>

<center>**<u>Modell 4:</u>**</center> <br>

```{r variable_importance_cv10_2.5k, eval = FALSE}
# get variable importance of model 4
var_imp_df_cv10_2.5k <- data.frame(Variable = rownames(varImp(model_knn_cv10_2.5k)[[1]]), Importance = varImp(model_knn_cv10_2.5k)[[1]]$Overall,
                                  Rank = rank(varImp(model_knn_cv10_2.5k)[[1]]$Overall)) %>%
  arrange(desc(Rank))
```

```{r var_imp_cv10_2.5k_table}
# display variable importance of model 4
kable(var_imp_df_cv10_2.5k[, 1:2] %>% head(5), "html") %>%
  kable_styling("striped")
```

<center>**$Laufzeit \approx 5 \space Stunden \space 57 \space Minuten$**</center>










